<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="robots" content="noindex">
    <link rel="stylesheet" href="../../css/style.css" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/slick-carousel@1.8.1/slick/slick-theme.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/slick-carousel@1.8.1/slick/slick.min.css">
    <link rel="icon" type="image/x-icon" href="../../favicon.ico">
    <title>My First Web Pag</title>
<script src="../../js/main.js"></script>
</head>
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.4/dist/jquery.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/slick-carousel@1.8.1/slick/slick.min.js"></script>
<script src="../../js/stars.js"></script>
<script>
    $(document).ready(function () {
    $('.slider').slick({
        autoplay: true,
        dots: true,
        arrows: false,
        fade: true,
        autoplaySpeed: 5000,
    });
});

// ハンバーガーメニューの制御
document.addEventListener('DOMContentLoaded', function() {
    const hamburger = document.getElementById('hamburger');
    const gnav = document.getElementById('gnav');
    
    if (hamburger && gnav) {
        hamburger.addEventListener('click', function() {
            hamburger.classList.toggle('active');
            gnav.classList.toggle('active');
        });
        
        // メニュー項目がクリックされたときにメニューを閉じる
        const menuLinks = gnav.querySelectorAll('a');
        menuLinks.forEach(link => {
            link.addEventListener('click', function() {
                hamburger.classList.remove('active');
                gnav.classList.remove('active');
            });
        });
    }
});
</script>
<body>
    <div class="container">
    <header>
        <div class="header-logo"><a href="../../index.html">Rs's Website</a></div>
        <div class="hamburger" id="hamburger">
            <span></span>
            <span></span>
            <span></span>
        </div>
        <nav class="gnav" id="gnav">
            <ul class="menu">
                <li><a href="../../index.html#about">About</a></li>
                <li><a href="../../index.html#skills">Skills</a></li>
                <li><a href="../projectslist.html">Projects</a></li>
                <li><a href="../articleslist.html">Articles</a></li>
            </ul>
            <div class="social-menu">
                <a href="https://x.com/rs_brzi" target="_blank" rel="noopener noreferrer" class="social-link twitter" aria-label="X (Twitter)">
                    <svg width="36" height="36" viewBox="0 0 24 24" fill="currentColor">
                        <path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"/>
                    </svg>
                </a>
                <a href="https://github.com/hana-rs" target="_blank" rel="noopener noreferrer" class="social-link github" aria-label="GitHub">
                    <svg width="36" height="36" viewBox="0 0 24 24" fill="currentColor">
                        <path d="M12 2C6.477 2 2 6.484 2 12.017c0 4.425 2.865 8.18 6.839 9.504.5.092.682-.217.682-.483 0-.237-.008-.868-.013-1.703-2.782.605-3.369-1.343-3.369-1.343-.454-1.158-1.11-1.466-1.11-1.466-.908-.62.069-.608.069-.608 1.003.07 1.531 1.032 1.531 1.032.892 1.53 2.341 1.088 2.91.832.092-.647.35-1.088.636-1.338-2.22-.253-4.555-1.113-4.555-4.951 0-1.093.39-1.988 1.029-2.688-.103-.253-.446-1.272.098-2.65 0 0 .84-.27 2.75 1.026A9.564 9.564 0 0112 6.844c.85.004 1.705.115 2.504.337 1.909-1.296 2.747-1.027 2.747-1.027.546 1.379.202 2.398.1 2.651.64.7 1.028 1.595 1.028 2.688 0 3.848-2.339 4.695-4.566 4.943.359.309.678.92.678 1.855 0 1.338-.012 2.419-.012 2.747 0 .268.18.58.688.482A10.019 10.019 0 0022 12.017C22 6.484 17.522 2 12 2z"/>
                    </svg>
                </a>
            </div>
        </nav>
    </header>
    <main class="main">
    <div class="main-contents">
    <div class="back-button">
        <a href="../articleslist.html">Articles一覧に戻る</a>
    </div>
    <p class="day">2025.7.21</p>
    <h2 class="individual-title">機械学習 講義ノートまとめ</h2>
    <h2 class="subtitle">第2回 機械学習の種類</h2>
    <div class="article-content">
        <p><b>そもそも機械学習って何？</b><br>
        すごくシンプルに言うと、機械学習とは、コンピューターにデータの中からパターンを見つけさせることです。<br>
        人間がプログラムで細かいルールを教える代わりに、モデル（数式のようなもの、例：y=f(x∣w)）と大量のデータをコンピューターに与えます。<br>
        すると、コンピューターがそのモデルをうまく機能させるための最適なパラメータ（数式のwの部分）を「学習」してくれるんです。<br>
        グラフ上のたくさんの点の集まりを見せて、これに一番フィットする直線をコンピューターに探させるようなイメージです。その直線の式を見つけ出すのが、コンピューターの仕事になります。</p>
        <hr>
        <p><b>機械学習の3つの種類</b><br>
        機械学習は、持っているデータの種類や目的によって、いくつかのタイプに分かれます。主なものは「教師あり学習」「教師なし学習」「強化学習」の3つです。</p>
        <ol>
            <li><b>教師あり学習</b><br>
                教師あり学習では、「ラベル付きデータ」（入力xと、それに対する正解の出力yがセットになったデータ）を使います。入力と出力の関係性を学習して、新しい未知の入力データに対して、正しい出力を予測できるようにすることが目的です。<br>
                <ul>
                    <li>回帰：出力（y）が連続的な数値の場合に使います。<br>目的: 数値を予測すること。<br>例: 株価の予測、売上予測など。</li>
                    <li>分類：出力（y）がカテゴリやクラス（「AかBか」のような離散値）の場合に使います。<br>目的: 入力データにラベルを付けること。<br>例: 迷惑メールの分類、画像認識など。</li>
                </ul>
            </li>
            <li><b>教師なし学習</b><br>
                教師なし学習では、「ラベルなしデータ」（正解の出力yがなく、入力xだけのデータ）を使います。データそのものの中に隠れているパターンや構造を見つけ出すのが目的です。<br>
                <ul>
                    <li>クラスタリング：これが主なタスクです。<br>目的: 似ているデータをグループ分け（分類）すること。<br>例: 顧客の分類、遺伝子解析など。</li>
                </ul>
            </li>
            <li><b>強化学習</b><br>
                強化学習は、ある「環境」の中でエージェント（コンピューター）が試行錯誤をします。「行動」をすると、それに対して「報酬」か「罰」が与えられ、長期的に見て合計の報酬が最大になるような「行動」のパターンを学習します。<br>
                <ul>
                    <li>目的: 最も多くの報酬を得るための長期的な戦略を学ぶこと。<br>例: ゲームのAI、ロボットの制御など。</li>
                </ul>
            </li>
        </ol>
        <hr>
    </div>
    
        <h2 class="subtitle">第3回 回帰その1：基本的事項</h2>
        <div class="article-content">
        <p><b>線形回帰と最小二乗法</b><br>
        線形回帰は、データ関係を直線の式で表す手法です。<br>
        ・単回帰: 説明変数が1つ (y = w₀ + w₁x₁)。<br>
        ・重回帰: 説明変数が複数 (y = w₀ + w₁x₁ + w₂x₂ + ...)。<br>
        最適な直線は「最小二乗法」で見つけます。これは、「実際のデータ点と、モデルの直線とのズレ（損失）の二乗の合計が、最も小さくなる」ようにパラメータwを決める方法です。</p>
        <hr>
        <p><b>なぜ「最小二乗法」が良いのか？</b><br>
        その根拠は「最尤法（さいゆうほう）」という考え方にあります。<br>
        1. 現実のデータには必ずノイズ（誤差）が含まれます。<br>
        2. このノイズは、プラスやマイナスの誤差がランダムに重なった結果なので、「平均0の正規分布（きれいな釣り鐘型の分布）に従う」と仮定するのが合理的です。<br>
        3. この仮定のもとで、「手元のデータが観測される確率（尤度）が最大になる」ような直線を求めると、結果的に、それが「ズレの二乗和（損失関数）を最小にする」ことと全く同じになります。</p>
        <hr>
        <p><b>モデルの評価指標</b><br>
        作ったモデルが良いかを評価する指標です。<br>
        ・MSE (Mean Square Error): 平均二乗誤差。ズレの二乗の平均値。<br>
        ・R² (決定係数): 「データのばらつきのうち、どれくらいの割合をモデルで説明できているか」を表します。1に近いほど良いモデルです。</p>
        <hr>
        </div>
        <h2 class="subtitle">第4回 回帰その2：過学習と正則化</h2>
        <div class="article-content">
        <p><b>回帰の悩みどころと「過学習」</b><br>
        ・多重共線性: 説明変数同士が似すぎていると、パラメータがうまく決まらない問題。<br>
        ・過学習（かがくしゅう）: モデルを複雑にしすぎて、訓練データのノイズまで学習してしまうこと。その結果、未知のデータに対する予測精度（汎化性能）が落ちてしまいます。</p>
        <hr>
        <p><b>解決策1：交差検証</b><br>
        交差検証（こうさけんしょう）は、汎化性能を正しく測るための手法です。データをk個に分割し、「(k-1)個で学習 → 1個でテスト」をk回繰り返して、性能の平均を評価します。これにより、過学習していないかを確認し、最適なモデルの複雑さを選べます。</p>
        <hr>
        <p><b>解決策2：正則化</b><br>
        正則化（せいそくか）は、損失関数に「パラメータwが大きくなりすぎたらペナルティを課す」項を追加して、過学習を防ぐ手法です。<br>
        ・Ridge（リッジ）回帰: パラメータwの二乗をペナルティに使います（L2正則化）。wの値を全体的に小さく抑えます。<br>
        ・Lasso（ラッソ）回帰: パラメータwの絶対値をペナルティに使います（L1正則化）。不要な説明変数のwをピッタリ0にできるため、変数選択も自動で行ってくれます。</p>
        <hr>
        </div>
        <h2 class="subtitle">第5回 識別その1：基本的事項</h2>
        <div class="article-content">
        <p><b>識別と決定境界</b><br>
        識別（分類）とは、データがどのクラスに属するかを当てることです。そのために、データ空間に「決定境界」という線を引きます。</p>
        <hr>
        <p><b>線形識別とロジスティック回帰</b><br>
        線形識別は、決定境界として<strong>まっすぐな直線（平面）</strong>を引く手法です。最小二乗法をそのまま使うと、外れ値に弱いという問題があります。そこで、分類専用の「ロジスティック回帰」を使います。<br>
        ・確率で考える: シグモイド関数を使い、モデルの出力を0から1の確率に変換します。<br>
        ・適切な損失関数: 確率を扱うのに適した「交差エントロピー」という損失関数で学習します。</p>
        <hr>
        <p><b>識別の評価指標</b><br>
        分類モデルの性能は、混同行列を使って評価します。<br>
        ・適合率 (Precision): TP / (TP + FP)。<br>「ポジティブと予測した中で、本当にポジティブだった割合」。<br>
        ・再現率 (Recall): TP / (TP + FN)。<br>「本当にポジティブなもののうち、どれだけ見つけられたかの割合」。<br>
        ・F1スコア: 適合率と再現率のバランスをとった指標です。<br>
        ・AUC: モデルの識別能力そのものを表す指標で、1に近いほど高性能です。</p>
        <hr>
        </div>
        <h2 class="subtitle">第6回：サポートベクターマシン (SVM)</h2>
        <div class="article-content">
        <p>前回までの線形識別では、決定境界は「直線」でした。しかし、データはいつも直線で綺麗に分けられるわけではありません。SVMは、より賢く、そして柔軟な決定境界を引くための手法です。</p>
        <hr>
        <p><b>SVMの基本アイデア：マージン最大化</b><br>
        SVMの最も重要な考え方は「マージン最大化」です。マージンとは、決定境界と最も近いデータ点との「距離」のことです。<br>
        SVMは、2つのクラスを分離する境界線を引くときに、このマージンが最も広くなるように境界線を決定します。マージンを広く取ることで、未知のデータに対する予測性能（汎化性能）が高くなると期待できます。<br>
        ・サポートベクトル: マージンを決めるのに使われる、境界に最も近いデータ点のことです。SVMという名前の由来にもなっています。<br>
        ・ソフトマージン: 実際にはデータが完全に分離できないことも多いため、いくつかのデータ点の誤分類を許容することで、より現実的な決定境界を見つける仕組みです。</p>
        <hr>
        <p><b>カーネルトリック：曲がった境界線を作る魔法</b><br>
        SVMのすごいところは、「カーネルトリック」というテクニックを使うことで、非線形（曲がった）決定境界を作れることです。<br>
        これは、元のデータ空間では直線で分離できなくても、データを高次元の空間に写像（変換）して、そこでなら直線で分離できるようにするという考え方です。カーネル関数を使うことで、この複雑な計算を効率的に行うことができます。</p>
        <hr>
        </div>
        <h2 class="subtitle">第7回：決定木とアンサンブル学習</h2>
        <div class="article-content">
        <p><b>決定木 (Decision Tree)</b><br>
        決定木は、「もし〇〇が××以上ならA、そうでなければB」といった、シンプルなルールを木の枝のように連ねてデータを分類する手法です。<br>
        ・作り方: 「ジニ不純度」などの指標を使い、データが最も綺麗に分かれるような質問（分岐）を上から順番に作っていきます。<br>
        ・特徴:<br>ルールが直感的で解釈しやすい。<br>何もしないと訓練データに適合しすぎて過学習しやすい。</p>
        <p><b>ジニ不純度とは？</b><br>
        ジニ不純度とは、決定木がデータを分割する際に、その分割がどれだけ良いかを測るための指標です。具体的には、あるノード（データのグループ）に、どれだけ異なるクラスのデータが混ざり合っているかを表します。<br>
        ・ジニ不純度が0 (最小) ✨: ノード内のデータが1つのクラスのみで構成されている「純粋」な状態です。誤分類の確率がゼロであることを意味します。<br>
        ・ジニ不純度が大きい: ノード内に異なるクラスが均等に近い割合で混ざり合っている状態です。<br>
        決定木は、このジニ不純度が最も小さくなるような分割ルールを探していきます。</p>
        <p><b>計算方法 🧮</b><br>
        あるノード t におけるジニ不純度 G(t) は、以下の式で計算されます。<br>
        G(t)=1−∑k=1KP(Ck∣t)2<br>
        ここで、K はクラスの総数です。P(Ck∣t) は、ノード t の中で、データがクラス k に属する確率です。これは、ノード t に含まれる全データ数 N(t) のうち、クラス k のデータ数 Nk(t) が占める割合で計算されます (Nk(t)/N(t))。</p>
        <p><b>具体例</b><br>
        ・例1：純粋なノード 5個のデータがすべてクラス1の場合： P(C1∣t)=5/5=1 G(t)=1−(12)=0<br>
        ・例2：不純なノード 5個のデータのうち、3個がクラス1、2個がクラス2の場合： P(C1∣t)=3/5, P(C2∣t)=2/5 G(t)=1−{(3/5)2+(2/5)2}=1−(9/25+4/25)=12/25=0.48</p>
        <p><b>決定木での使われ方</b><br>
        決定木は、各ステップでジニ不純度が最も大きく減少するような分割ルールを探します。この不純度の減少量を ΔG(t) と呼びます。<br>
        ΔG(t)=G(t親)−(pLG(t左子)+pRG(t右子))<br>
        ・G(t親): 分割前の親ノードのジニ不純度<br>
        ・G(t左子),G(t右子): 分割後の左右の子ノードのジニ不純度<br>
        ・pL,pR: それぞれ左右の子ノードにデータが分配される割合<br>
        アルゴリズムは、考えられるすべての分割パターン（どの説明変数で、どの値で区切るか）についてこの ΔG(t) を計算し、この値が最大になる分割ルールを採用します。これを繰り返して木を成長させていきます。</p>
        <hr>
        <p><b>アンサンブル学習：三人寄れば文殊の知恵</b><br>
        決定木は単体だと過学習しやすい弱点がありますが、「アンサンブル学習」という手法と組み合わせることで非常に強力なモデルになります。これは、性能がそこそこの弱い学習器（弱学習器）をたくさん集めて、多数決で最終的な判断を下すことで、全体の性能を向上させる考え方です。</p>
        <p><b>モデルの誤差：「バイアス」と「バリアンス」</b><br>
        モデルの性能を考える上で、「バイアス」と「バリアンス」という2つの誤差の概念が重要です。<br>
        ・バイアス: モデルの予測が、平均的にどれだけ真の値からズレているか（勘違いの度合い）。<br>
        ・バリアンス: データセットが変わるごとに、モデルの予測がどれだけばらつくか（不安定さの度合い）。<br>
        ・一般に、この2つはトレードオフの関係にあります。</p>
        <p><b>アンサンブル学習の種類</b><br>
        ・バギング (Bagging): バリアンスが大きい（過学習しやすい）弱学習器をたくさん使い、それらの結果を平均（多数決）することで、全体のバリアンスを抑える手法です。<br>
        o ブートストラップ: 元のデータからランダムに（重複を許して）サンプリングし、複数の異なる訓練データセットを作ります。<br>
        o ランダムフォレスト: バギングの考え方をベースに、決定木の分岐を作る際に説明変数もランダムに一部だけ選ぶことで、決定木同士の相関を低くし、より効果的にバリアンスを下げる手法です。<br>
        ・ブースティング (Boosting): バイアスが大きい（単純すぎる）弱学習器を使い、1つ目の学習器が間違えた問題を、2つ目の学習器が重点的に学ぶ、というように学習器を直列につなげていく方法です。間違えやすい部分をどんどん補強していくことで、全体のバイアスを改善します。</p>
        <hr>
        </div>
        <h2 class="subtitle">第8回 教師なし学習その1：基本的事項</h2>
        <div class="article-content">
        <p><b>教師なし学習とは？</b><br>
        教師なし学習は、正解の出力（ラベル）がない、入力データだけを使って、そのデータの中に隠れているパターンや構造を学習する手法です。<br>
        ・目的: データの分類（クラスタリング）、可視化、異常検知など。<br>
        ・データ: ラベルなしデータ。</p>
        <hr>
        <p><b>K-meansクラスタリング</b><br>
        K-meansは、ラベルのないデータを、似たもの同士であらかじめ決めたK個のグループ（クラス）に分ける、最も代表的なクラスタリング手法です。</p>
        <p><b>K-meansのアルゴリズム</b><br>
        K-meansは、以下の手順を繰り返して、最適なクラスタを見つけます。<br>
        1. 初期化: 各クラスの中心点（セントロイド）をランダムに決めます。<br>
        2. 分類の更新: 各データ点を、最も距離が近いセントロイドのクラスに割り当てます。<br>
        3. セントロイドの更新: 新しく割り当てられたクラスのデータ点の平均を計算し、セントロイドの位置を更新します。<br>
        4. 繰り返し: セントロイドの位置が動かなくなるまで、2と3を繰り返します。<br>
        このアルゴリズムは、「各クラス内のセントロイドからの距離の二乗和」を最小化することを目指しています。</p>
        <p><b>Kの決め方：エルボー法</b><br>
        最適なクラスタ数Kを決める方法として「エルボー法」がよく使われます。これは、Kの数を1, 2, 3, ...と増やしていきながら、それぞれの損失をグラフにプロットする方法です。損失の減少具合が急に緩やかになる点（グラフが肘のように曲がる点）を最適なKと判断します。</p>
        <hr>
        <p><b>K-meansの注意点</b><br>
        ・初期値依存: アルゴリズムは、最初にランダムに選んだセントロイドの位置に結果が依存してしまいます。そのため、異なる初期値で何回か実行し、最も損失が小さくなった結果を採用するのが一般的です。<br>
        ・形状の仮定: K-meansは、各クラスタが球形（等方的）であることを仮定しているため、細長いデータや複雑な形のクラスタを見つけるのは苦手です。</p>
        <hr>
        </div>
        <h2 class="subtitle">第9回 教師なし学習その2：生成モデル入門</h2>
        <div class="article-content">
        <p><b>生成モデルと潜在変数</b><br>
        生成モデルとは、手元にあるデータがどのような確率分布から生まれてきたのか（生成されたのか）をモデル化する手法です。この分布を学習することで、本物のデータに似た新しいデータを生成することが可能になります。<br>
        しかし、現実のデータは単純な確率分布では表現できないほど複雑です。そこで、モデルの表現力を上げるために「潜在変数（せんざいへんすう）」という考え方を導入します。<br>
        ・潜在変数 (z): 観測データ (x) からは直接見えないけれど、そのデータの背後にあって、データの性質を決めている変数です。例えば、手書き数字の画像データ (x) の裏には、「どの数字か」「どれくらい傾いているか」といった潜在変数 (z) がある、と考えることができます。</p>
        <hr>
        <p><b>EMアルゴリズム</b><br>
        潜在変数を含むモデルのパラメータを学習するのは複雑です。そこで使われるのが「EMアルゴリズム」です。これは、直接最大化するのが難しい尤度の代わりに、ELBO（下限）と呼ばれる量を最大化する手法です。以下の2つのステップを繰り返すことで、パラメータを最適化します。<br>
        ・Eステップ (Expectation): 現在のパラメータを使って、観測データから潜在変数がどうなっているかを推定します。具体的には、事後確率 p(z∣x) を計算します。<br>
        ・Mステップ (Maximization): Eステップで推定した潜在変数を「仮の正解」とみなして、完全データの尤度が最大になるようにモデルのパラメータを更新します。<br>
        この2つのステップを繰り返すことで、モデルはデータにより適合していきます。</p>
        <hr>
        <p><b>混合ガウスモデル (Gaussian Mixture Model; GMM)</b><br>
        GMMは、EMアルゴリズムを使った代表的なクラスタリング手法です。<br>
        <br>・考え方:<br>データは、複数の正規分布（ガウス分布）が混ざり合ってできていると仮定します。<br>各正規分布がそれぞれ一つのクラスタに対応します。<br>
        <br>・潜在変数:<br>各データが、K個ある正規分布のどれから生成されたかを示す変数を潜在変数として使います。<br>
        <br>・ソフトクラスタリング:<br>K-meansが各データを一つのクラスタに割り当てる（ハードクラスタリング）のに対し、GMMは「データAは80%の確率でクラスタ1に属し、20%の確率でクラスタ2に属す」というように、<strong>確率的に分類（ソフトクラスタリング）</strong>します。
        </p>
        <p><b>K-meansとの違い</b><br>
        GMMは、各正規分布の共分散行列を推定するため、K-meansが苦手とする細長い楕円形のクラスタも捉えることができます。<br>K-meansは、GMMの各正規分布の分散を0に近づけた特別な場合と考えることができます。
        </p>
        <hr>
        </div>
        <h2 class="subtitle">第10回 教師なし学習その3：階層的クラスタリングと主成分分析</h2>
        <div class="article-content">
        <p><b>階層的クラスタリング</b><br>
        K-meansやGMMは、データが球形や楕円形の塊であることを前提としていました。<br>階層的クラスタリングは、より複雑な形状のデータにも対応できる手法です。<br>
        <br>・特徴:<br>事前にクラスタ数Kを決める必要がありません。<br>最も近いデータ（またはクラスタ）から順番に、一つずつまとめていくボトムアップの手法です。<br>クラスタがまとまっていく過程を<strong>デンドログラム（系統樹）</strong>という木構造で可視化できます。
        </p>
        <p><b>クラスタ間の距離の決め方</b><br>
        どのクラスタ同士を「近い」と判断するかの基準（連結法）には、いくつか種類があります。<br>
        <br>・単連結 (Single Linkage):<br>2つのクラスタ間で、最も近いデータ点同士の距離をクラスタ間の距離とします。細長いクラスタの検出が得意です。<br>
        <br>・完全連結 (Complete Linkage):<br>2つのクラスタ間で、最も遠いデータ点同士の距離をクラスタ間の距離とします。コンパクトなクラスタを生成しやすいです。<br>
        <br>・ウォード法 (Ward Linkage):<br>2つのクラスタをまとめた時に、クラスタ内の分散（中心からの距離の二乗和）がどれだけ増加するかを基準にします。一般的に精度が高いとされています。
        </p>
        <hr>
        <p><b>主成分分析 (Principal Component Analysis; PCA)</b><br>
        <strong>主成分分析 (PCA)</strong>は、次元縮約の代表的な手法です。<br>高次元で可視化できないデータを、情報をなるべく失わずに低次元（2次元や3次元）に圧縮して、人間が理解しやすくするために使われます。
        </p>
        <p><b>PCAの基本アイデア</b><br>
        PCAは、データが最もばらついている方向（分散が最大になる方向）を新しい軸として見つけ出します。<br>
        <br>1. 第1主成分:<br>データの分散が最大になる方向を見つけます。<br>
        2. 第2主成分:<br>第1主成分と直交する方向の中で、次に分散が最大になる方向を見つけます。<br>
        3. これを繰り返し、新しい座標系（主成分）を作ります。
        </p>
        <p><b>寄与率：どの主成分がどれだけ重要か？</b><br>
        主成分分析では、新しい軸（主成分）が元のデータの情報をどれだけ持っているかを評価するために「寄与率」という指標を使います。<br>
        <br>・寄与率 (Explained Variance Ratio):<br>各主成分が、元のデータ全体のばらつき（総分散）のうち、どれくらいの割合を説明できているかを示す値です。<br>
        ・累積寄与率:<br>第1主成分から順に寄与率を足し合わせた値です。<br>
        この累積寄与率を見ることで、「最初の2つの主成分だけで、元のデータの情報の90%を説明できている」といった判断ができます。<br>これにより、情報をほとんど失うことなく、高次元のデータを2次元に削減して可視化する、といったことが可能になります。
        </p>
        <hr>
        </div>
        <h2 class="subtitle">第11回 ニューラルネットその1：基本的事項</h2>
        <div class="article-content">
        <p><b>ニューラルネットの基本アイデア</b><br>
        これまでの線形モデルは、データを「一様に」変形させることしかできませんでした。線を引いたり、全体を傾けたり、といった具合です。これでは複雑なデータに対応できません。<br>
        そこで、線形モデルの間に「活性化関数」という非線形な関数を挟み込みます。<br>
        ・線形モデル: u=Wx+b<br>
        ・活性化関数: σ(u) （シグモイド関数やReLU関数など）<br>
        この「線形モデル＋活性化関数」のセットが、ニューラルネットワークの基本単位（ニューロン）となります。</p>
        <hr>
        <p><b>多層化の威力：なぜ層を重ねるのか？</b><br>
        線形モデルだけを何層重ねても、結局は一つの大きな線形モデルにしかなりません。しかし、間に活性化関数を挟んで多層化すると、モデルの表現力が劇的に向上します。<br>
        ・1層目: データをある方向に潰したり、曲げたりする。<br>
        ・2層目: 1層目で変形されたデータを、さらに別の方向に変形させる。<br>
        ・これを繰り返す: 層を重ねるごとに、データはどんどん複雑な形に変形されていきます。<br>
        これにより、元の空間ではぐちゃぐちゃで分離できなかったデータも、ニューラルネットを通すことで、最終的には単純な直線で分離できる形に変換することができるのです。</p>
        <hr>
        <p><b>ニューラルネットワークの構造</b><br>
        ニューラルネットワークは、層（レイヤー）を重ねて構成されます。<br>
        ・入力層: 最初のデータを受け取る層。<br>
        ・中間層（隠れ層）: 入力層と出力層の間にある層。この層の数（深さ）や、各層のニューロンの数（幅）を増やすことで、モデルの表現力をコントロールします。<br>
        ・出力層: 最終的な予測結果を出す層。<br>
        ニューラルネットワークは、この<strong>「何層もの合成関数のオバケ」</strong>と考えることができます。</p>
        <hr>
        </div>
        <h2 class="subtitle">第12回 ニューラルネットその2：ニューラルネットの学習</h2>
        <div class="article-content">
        <p><b>パラメータの学習方法：勾配降下法</b><br>
        ニューラルネットワークは非常に複雑な非線形モデルなので、線形回帰のように数式を解いて一発で最適なパラメータを求めることはできません。<br>
        そこで、勾配降下法 (Gradient Descent) という、コンピュータを使った数値計算の手法を用います。これは、山を少しずつ下りていくように、<strong>損失関数が最も急になる方向（勾配）</strong>に沿ってパラメータを繰り返し更新し、損失が最小になる地点を探す方法です。<br>
        更新式は以下のようになります。<br>
        W←W−η∇L(W)<br>
        ・∇L(W): 損失関数のパラメータ W に関する勾配（微分）<br>
        ・η: 学習率。パラメータを一度にどれだけ更新するかを決める値</p>
        <hr>
        <p><b>勾配の計算方法：誤差逆伝播法</b><br>
        ニューラルネットワークは「合成関数のオバケ」なので、パラメータに関する勾配（微分）を真面目に計算しようとすると、非常に複雑になります。<br>
        この複雑な計算を効率的に行うためのアルゴリズムが「誤差逆伝播法 (Backpropagation)」です。<br>
        これは、合成関数の微分（連鎖律）を使い、出力側から入力側へと逆向きに誤差に関する微分値を伝播させていくことで、一度の計算で全てのパラメータに関する勾配を効率よく求める手法です。</p>
        <hr>
        <p><b>学習における高度な話題</b><br>
        勾配降下法にはいくつかの課題があります。<br>
        ・局所解: 損失関数の谷（最小値）が複数ある場合、一番低い谷（大域解）ではなく、途中の浅い谷（局所解）にはまってしまうことがあります。<br>
        o 対策: 確率的勾配降下法 (SGD) を使います。これは、データの一部（ミニバッチ）をランダムに選んで勾配を計算する方法で、勾配が「揺らぐ」ことで局所解から脱出しやすくなります。<br>
        ・勾配消失: 層が深くなると、逆伝播で伝わる勾配がどんどん小さくなり、最終的に0に近くなってしまう問題です。これにより、入力層に近い部分の学習が進まなくなります。<br>
        o 対策: ReLU関数の使用や、スキップ接続（ResNetで導入）といったネットワーク構造の工夫が有効です。</p>
    </div>
    </div>
    <footer>
    <div class="footer-copyright">© 2024-2025 hana-rs<br>All Rights Reserved.</div>
    </footer>
</main>
</div>
</body>
</html>
